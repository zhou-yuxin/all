# 支持reserve操作的纯逻辑内存分配器

与jemalloc、ptmalloc以及tcmalloc等相似的是，plmalloc也是一个内存分配器。但是plmalloc最大的特点在于，它是一个支持**reserve操作**的**纯逻辑**内存分配器。

## 纯逻辑
大多数内存分配器，返回的都是可以直接操作的内存地址。也就说说，内存分配器不仅实现了分配算法，还负责了向操作系统申请内存的职责。但是，随着异构内存（DRAM与NVM混合系统）的出现，“向操作系统申请内存”这个职责变得多样化，比如“向操作系统申请DRAM空间”与“向操作系统申请NVM空间”需要使用不一样的API。甚至，NVM可以被配置成不同模式（[参考PMDK](https://pmem.io/ndctl/)），不同模式下的用法也不一样。

这让我意识到，必须将“分配算法”与“内存申请”解耦。可是，修改现有的内存分配器太过复杂，后期也不利于维护，因此想开发一个自己的内存分配器。这也是plmalloc的初衷。

从plmalloc的视角看，用户已经分配好了一大块连续内存（假设1GB），它的任务就是提供指导，即回答“如何把1GB的空间分为不同大小的切片（比如64字节，128字节等等）”。这是一个纯算法问题，不需要涉及真实的内存申请。因此，plmalloc的返回值也是一个个的整数，即“分配的内存”相对于这个1GB连续空间开头的偏移量。对于用户，真正操作的内存地址就是这1GB内存空间的首地址加上该偏移量。

因此，plmalloc不关心内存是什么类型、什么形式，从而做到纯逻辑。

## reserve操作
大多数内存分配器都是为DRAM设计的，所以不考虑如何恢复数据。但是，plmalloc可能是在分配NVM，用户可能希望，当进程重启后可以恢复到上次进程结束时的内存状态。

举例而言，机器上装配了一根4GB的NVM内存条。用户申请了1GB的NVM空间，其对应NVM内存条上最末尾的1GB，而其在地址空间中的首地址是0x10000。用户通过plmalloc分配了一个32字节的切片和一个1024字节的切片（偏移量分别是0x100和0x5000），并在里面写入数据A和B，然后结束进程。

当用户下次运行该进程时，将NVM内存条上最末尾的1GB映射到了地址空间中的0x20000的位置。由于上次在偏移量0x100的地方有一段32字节的数据A，在偏移量0x5000的地方有一段1024字节的数据B，因此用户访问0x20100可以得到数据A，访问0x25000可以得到数据B。也就是说，数据本身是不会丢失的。

但是，分配状态丢失了！当用户再次调用plmalloc分配内存时，如果分配的切片与A或B有重叠部分，那么A和B很可能会被新的数据覆盖。显然，这不是用户希望的结果。用户希望的是，plmalloc能恢复到上次进程中的分配状态，知晓A = [0x100, 0x1200)和B = [0x5000, 0x5400)的两个切片是已经分配的。

“恢复分配状态”是所有面向NVM的内存分配器需要解决的问题，而解决方法有二。第一种，也是[libpmemobj](https://pmem.io/pmdk/libpmemobj/)的做法，即分配器把meta data（即分配信息）存储在NVM上，这样就不会丢失。但是该方案中，由于meta data是频繁修改的，因此会大大增加NVM的读写压力。而NVM本来就带宽、延迟大，于是内存分配动作会严重拖累整体性能。第二种，是由业务逻辑自身记录状态。比如我们之前改的Key-Value分离的RocksDB中，对于较大的Value，则是放到NVM上，然后用NVM上的偏移量代替value送入RocksDB中。当需要查询某个Key时，得到了其偏移量，再去访问NVM得到具体的Value。这种情况下，遍历所有RocksDB中的Key-Value对，就能知道哪些切片是需要恢复的。而分配器需要提供reserve接口，将各个有效切片的偏移量和长度传给reserve接口，让分配器将这些切片所在区域设置为已分配状态。该方案的好处在于，内存分配器的meta data可以放在DRAM，从而使得运行时内存管理很迅速（但是恢复时可能更加耗时）。

## 架构
架构上还是沿用了经典的内存分配器架构，即buddy + bitmap。

最底层是一个伙伴分配器（buddy allocator），只能分配2<sup>n</sup>个单元。在具体代码中，将“单元”称作“块（block)”，一个块是连续的512字节。因此，伙伴分配器只能分配2<sup>n</sup>*512字节的切片。当然，[buddy.h](include/buddy.h)和[buddy.c](src/buddy.c)本身是不知道块大小的，也不需要知道块大小。伙伴分配器的实现也不需要太多赘述，百度一下其原理，就能写出算法了。

显然，伙伴分配器的粒度太粗糙了，会导致大量的内存浪费。因此，在伙伴分配器之上，为每个块绑定了一个bitmap。该bitmap使用一个uint64_t表示，好处在于使用一条**ctz**指令，就能找到bitmap中最低位的'1'在哪里。

各个大小等级的划分如下：
等级|可用大小
--|--
0, 1, 2, 3|8, 10, 12, 14
4, 5, 6, 7|16, 20, 24, 28
8, 9, 10, 11|32, 40, 48, 56
12, 13, 14, 15|64, 80, 96, 112
16, 17, 18, 19|128, 160, 192, 224
20, 21, 22, 23|256, 320, 384, 448
24, 25, 26, 27|512, 640, 768, 896
28, 29, 30, 31|1024, 1280, 1536, 1792
32, 33, 34, 35|2048, 2560, 3072, 3584
36, 37, 38, 39|4K, 5K, 6K, 7K
40, 41, 42, 43|8K, 10K, 12K, 14K
44, 45, 46, 47|16K, 20K, 24K, 28K
48, 49, 50, 51|32K, 40K, 48K, 56K
52, 53, 54, 55|64K, 80K, 96K, 112K
56, 57, 58, 59|128K, 160K, 192K, 224K
60, 61, 62, 63|256K, 320K, 384K, 448K

超过448K的对象将直接使用伙伴分配器进行分配，即512K, 1M, 2M, 4M, 8M, 16M。